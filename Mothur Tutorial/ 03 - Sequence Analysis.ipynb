{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Mothur tutorial <br/> Sequence Analysis</center>\n",
    "I will use **Mothur** to figure out what OUTs are present in a fecal sample of a mice and a Mock sample. This does not take too long, has a limited memory footprint but can be expanded to higher number of samples. I base this off of the Tutorial that can be found at https://www.mothur.org/wiki/MiSeq_SOP [1]. I diverge from it at the choice of data, as well as I will use the up to date reference files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data processing\n",
    "**Mothur**  is a full data analysis suite, so it can attach to your research pipeline at the point where raw reads are produced.\n",
    "\n",
    "** \\*\\* Note that you need to have the Mothur() Bash function (see Installation/Usage part) set up to run this tutorial! **\n",
    "\n",
    "## 1\\. Contig assembly\n",
    "\n",
    "First we make **Mothur** discover pairended read files in the `Data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > make.file(inputdir=Data, type=gz, prefix=tutor)\n",
      "Setting input directory to: Data/\n",
      "Output File Names: \n",
      "Data/tutor.single.files\n",
      "Data/tutor.paired.files\n"
     ]
    }
   ],
   "source": [
    "Mothur \"make.file(inputdir=Data, type=gz, prefix=tutor)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a step is completed **Mothur** will generate the files instead of variables to save operative memory. The processes are largely sequential and generate a lot of files with **very** long names. There is no control over how these files are named after a function completes. Generally they make sense, but it is relatively difficult to keep track of them, this is the main reason why I elected to use Jupyter with a Bash kernel to do this tutorial. - It helps to keep together my workflow and the commands I use.\n",
    "\n",
    "It was greedy produced the pair ended reads `.fastq.gz` and the `HMP_MOCK.v35.fasta.gz` which is not a suitable file for contig assembly. **Mothur** believes this a sample as well, but since it is not a pairended FASTA file, we can fix this easily. We only want to keep the dictionary of samples which have paired read files. Over time the names of the files downstream in our process will have more and more additions separated by dots, so let's also rename the file we need to keep it simple. Otherwise **Mothur** will carry the `.parired` ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rm Data/tutor.single.files\n",
    "mv Data/tutor.paired.files Data/tutor.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ontu the real contig assembly, it is a single command. There ar options like `processors=2` that is worth using, I will not talk about fine tuning this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > make.contigs(file=Data/tutor.files, processors=2)\n",
      "Using 2 processors.\n",
      ">>>>>\tProcessing file pair Data/Mock_S280_L001_R1_001.fastq.gz - Data/Mock_S280_L001_R2_001.fastq.gz (files 2 of 2)\t<<<<<\n",
      "Making contigs...\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "4779\n",
      "Done.\n",
      "It took 4 secs to assemble 4779 reads.\n",
      "mothur > set.logfile(name=mothur.log, append=T)\n",
      "mothur > make.contigs(file=Data/tutor.files, processors=2)\n",
      "Using 2 processors.\n",
      ">>>>>\tProcessing file pair Data/F3D150_S216_L001_R1_001.fastq.gz - Data/F3D150_S216_L001_R2_001.fastq.gz (files 1 of 2)\t<<<<<\n",
      "Making contigs...\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "5509\n",
      "Done.\n",
      "It took 5 secs to assemble 5509 reads.\n",
      "It took 5 secs to process 10288 sequences.\n",
      "Group count: \n",
      "F3D150\t5509\n",
      "Mock\t4779\n",
      "Total of all groups is 10288\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.fasta\n",
      "Data/tutor.trim.contigs.qual\n",
      "Data/tutor.contigs.report\n",
      "Data/tutor.scrap.contigs.fasta\n",
      "Data/tutor.scrap.contigs.qual\n",
      "Data/tutor.contigs.groups\n",
      "[WARNING]: your sequence names contained ':'.  I changed them to '_' to avoid problems in your downstream analysis.\n",
      "mothur > quit()\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<^>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<^>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<^>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Detected 1 [WARNING] messages, please review.\n",
      "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<^>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "Mothur \"make.contigs(file=Data/tutor.files, processors=2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mothur** is very verbose, but we only care about the end of the output:\n",
    "\n",
    "`Group count: \n",
    "F3D150\t5509\n",
    "Mock\t4779\n",
    "Total of all groups is 10288\n",
    "Output File Names: \n",
    "Data/tutor.trim.contigs.fasta\n",
    "Data/tutor.trim.contigs.qual\n",
    "Data/tutor.contigs.report\n",
    "Data/tutor.scrap.contigs.fasta\n",
    "Data/tutor.scrap.contigs.qual\n",
    "Data/tutor.contigs.groups`\n",
    "\n",
    "We see that there are two groups (F3D150 and Mock) and next to them we see the number of contigs generated per group. Each contig corresponds to - hopefully - a V4 segment of 16S rRNA in the sample.\n",
    "Notice that a bunch of files have been created in the `Data` folder, which we will use for further processing.\n",
    "*The workflow with **Mothur** is such that files are produced in each step that will be used in the next step, whenever a file is generated it is listed under `Output File Names` at the end.*\n",
    "\n",
    "## 2\\. Contig analysis and filtering\n",
    "\n",
    "Let's see some statistics on the contigs! Note, that we do not deal with individual samples, but we process all the contings *together*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta=Data/tutor.trim.contigs.fasta)\n",
      "Using 1 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1\t249\t249\t0\t3\t1\n",
      "2.5%-tile:\t1\t252\t252\t0\t3\t258\n",
      "25%-tile:\t1\t252\t252\t0\t4\t2573\n",
      "Median: \t1\t253\t253\t0\t4\t5145\n",
      "75%-tile:\t1\t253\t253\t0\t5\t7717\n",
      "97.5%-tile:\t1\t254\t254\t5\t6\t10031\n",
      "Maximum:\t1\t502\t502\t249\t243\t10288\n",
      "Mean:\t1\t252.92\t252.92\t0.639386\t4.55978\n",
      "# of Seqs:\t10288\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.summary\n",
      "It took 0 secs to summarize 10288 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta=Data/tutor.trim.contigs.fasta)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting way of showing a histogram of the data. The column names should be self explanatory, except for `Ambigs` (maximum number of ambiguous bases), and `Polymer` (the longest homopolymer) per the category.\n",
    "\n",
    "We can see that most of the reads are 252-254bp long. We also see that there are contigs with the length of 502, these all contain many ambiguous calls. We can safely discard of the sequences with ambiguity and excessive length, because these arised from incorrect read pairing. To do this we must screen the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > screen.seqs(fasta=Data/tutor.trim.contigs.fasta, group=Data/tutor.contigs.groups, maxambig=0, maxlength=255, processors=2)\n",
      "Using 2 processors.\n",
      "Processing sequence: 100\n",
      "Processing sequence: 200\n",
      "Processing sequence: 300\n",
      "Processing sequence: 400\n",
      "Processing sequence: 500\n",
      "Processing sequence: 600\n",
      "Processing sequence: 700\n",
      "Processing sequence: 800\n",
      "Processing sequence: 900\n",
      "Processing sequence: 1000\n",
      "Processing sequence: 1100\n",
      "Processing sequence: 1200\n",
      "Processing sequence: 1300\n",
      "Processing sequence: 1400\n",
      "Processing sequence: 1500\n",
      "Processing sequence: 1600\n",
      "Processing sequence: 1700\n",
      "Processing sequence: 1800\n",
      "Processing sequence: 1900\n",
      "Processing sequence: 2000\n",
      "Processing sequence: 2100\n",
      "Processing sequence: 2200\n",
      "Processing sequence: 2300\n",
      "Processing sequence: 2400\n",
      "Processing sequence: 2500\n",
      "Processing sequence: 2600\n",
      "Processing sequence: 2700\n",
      "Processing sequence: 2800\n",
      "Processing sequence: 2900\n",
      "Processing sequence: 3000\n",
      "Processing sequence: 3100\n",
      "Processing sequence: 3200\n",
      "Processing sequence: 3300\n",
      "Processing sequence: 3400\n",
      "Processing sequence: 3500\n",
      "Processing sequence: 3600\n",
      "Processing sequence: 3700\n",
      "Processing sequence: 3800\n",
      "Processing sequence: 3900\n",
      "Processing sequence: 4000\n",
      "Processing sequence: 4100\n",
      "Processing sequence: 4200\n",
      "Processing sequence: 4300\n",
      "Processing sequence: 4400\n",
      "Processing sequence: 4500\n",
      "Processing sequence: 4600\n",
      "Processing sequence: 4700\n",
      "Processing sequence: 4800\n",
      "Processing sequence: 4900\n",
      "Processing sequence: 5000\n",
      "Processing sequence: 5100\n",
      "Processing sequence: 5140\n",
      "mothur > set.logfile(name=mothur.log, append=T)\n",
      "mothur > screen.seqs(fasta=Data/tutor.trim.contigs.fasta, group=Data/tutor.contigs.groups, maxambig=0, maxlength=255, processors=2)\n",
      "Using 2 processors.\n",
      "Processing sequence: 100\n",
      "Processing sequence: 200\n",
      "Processing sequence: 300\n",
      "Processing sequence: 400\n",
      "Processing sequence: 500\n",
      "Processing sequence: 600\n",
      "Processing sequence: 700\n",
      "Processing sequence: 800\n",
      "Processing sequence: 900\n",
      "Processing sequence: 1000\n",
      "Processing sequence: 1100\n",
      "Processing sequence: 1200\n",
      "Processing sequence: 1300\n",
      "Processing sequence: 1400\n",
      "Processing sequence: 1500\n",
      "Processing sequence: 1600\n",
      "Processing sequence: 1700\n",
      "Processing sequence: 1800\n",
      "Processing sequence: 1900\n",
      "Processing sequence: 2000\n",
      "Processing sequence: 2100\n",
      "Processing sequence: 2200\n",
      "Processing sequence: 2300\n",
      "Processing sequence: 2400\n",
      "Processing sequence: 2500\n",
      "Processing sequence: 2600\n",
      "Processing sequence: 2700\n",
      "Processing sequence: 2800\n",
      "Processing sequence: 2900\n",
      "Processing sequence: 3000\n",
      "Processing sequence: 3100\n",
      "Processing sequence: 3200\n",
      "Processing sequence: 3300\n",
      "Processing sequence: 3400\n",
      "Processing sequence: 3500\n",
      "Processing sequence: 3600\n",
      "Processing sequence: 3700\n",
      "Processing sequence: 3800\n",
      "Processing sequence: 3900\n",
      "Processing sequence: 4000\n",
      "Processing sequence: 4100\n",
      "Processing sequence: 4200\n",
      "Processing sequence: 4300\n",
      "Processing sequence: 4400\n",
      "Processing sequence: 4500\n",
      "Processing sequence: 4600\n",
      "Processing sequence: 4700\n",
      "Processing sequence: 4800\n",
      "Processing sequence: 4900\n",
      "Processing sequence: 5000\n",
      "Processing sequence: 5100\n",
      "Processing sequence: 5148\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.fasta\n",
      "Data/tutor.trim.contigs.bad.accnos\n",
      "Data/tutor.contigs.good.groups\n",
      "It took 0 secs to screen 10288 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"screen.seqs(fasta=Data/tutor.trim.contigs.fasta,\n",
    "                    group=Data/tutor.contigs.groups,\n",
    "                    maxambig=0,\n",
    "                    maxlength=255,\n",
    "                    processors=2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the new output file `tutor.trim.contigs.good.fasta` containing the sequences that passed the screen.\n",
    "We can check if this filtering was successful with sequence statistics again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta=Data/tutor.trim.contigs.good.fasta)\n",
      "Using 1 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1\t251\t251\t0\t3\t1\n",
      "2.5%-tile:\t1\t252\t252\t0\t3\t219\n",
      "25%-tile:\t1\t252\t252\t0\t4\t2185\n",
      "Median: \t1\t253\t253\t0\t4\t4369\n",
      "75%-tile:\t1\t253\t253\t0\t5\t6553\n",
      "97.5%-tile:\t1\t254\t254\t0\t6\t8519\n",
      "Maximum:\t1\t255\t255\t0\t6\t8737\n",
      "Mean:\t1\t252.699\t252.699\t0\t4.48083\n",
      "# of Seqs:\t8737\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.summary\n",
      "It took 1 secs to summarize 8737 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta=Data/tutor.trim.contigs.good.fasta)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file is free of ambiguities and the contigs have the expected read length.\n",
    "\n",
    "## 3. Sequence (contig) number reduction\n",
    "\n",
    "Our files contain duplicates which can be grouped and joined into single groups for faster downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > unique.seqs(fasta= Data/tutor.trim.contigs.good.fasta)\n",
      "1000\t358\n",
      "2000\t641\n",
      "3000\t893\n",
      "4000\t1121\n",
      "5000\t1324\n",
      "6000\t1458\n",
      "7000\t1582\n",
      "8000\t1685\n",
      "8737\t1774\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.names\n",
      "Data/tutor.trim.contigs.good.unique.fasta\n"
     ]
    }
   ],
   "source": [
    "Mothur \"unique.seqs(fasta= Data/tutor.trim.contigs.good.fasta)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need count the number of occurrences not to loose track of the frequency (weight) of each contig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > count.seqs(name=Data/tutor.trim.contigs.good.names, group=Data/tutor.contigs.good.groups)\n",
      "Using 1 processors.\n",
      "It took 0 secs to create a table for 8737 sequences.\n",
      "Total number of sequences: 8737\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.count_table\n"
     ]
    }
   ],
   "source": [
    "Mothur \"count.seqs(name=Data/tutor.trim.contigs.good.names,\n",
    "                   group=Data/tutor.contigs.good.groups)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can recall the same sequence statistic command on the file with the unique sequences (`tutor.trim.contigs.good.unique.fasta`) and the count table for the occurrence of each contig (`tutor.trim.contigs.good.count_table`) to recover the sequence statistics seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta= Data/tutor.trim.contigs.good.unique.fasta, count=Data/tutor.trim.contigs.good.count_table)\n",
      "Using 1 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1\t251\t251\t0\t3\t1\n",
      "2.5%-tile:\t1\t252\t252\t0\t3\t219\n",
      "25%-tile:\t1\t252\t252\t0\t4\t2185\n",
      "Median: \t1\t253\t253\t0\t4\t4369\n",
      "75%-tile:\t1\t253\t253\t0\t5\t6553\n",
      "97.5%-tile:\t1\t254\t254\t0\t6\t8519\n",
      "Maximum:\t1\t255\t255\t0\t6\t8737\n",
      "Mean:\t1\t252.699\t252.699\t0\t4.48083\n",
      "# of unique seqs:\t1774\n",
      "total # of seqs:\t8737\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.summary\n",
      "It took 0 secs to summarize 8737 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta= Data/tutor.trim.contigs.good.unique.fasta,\n",
    "                     count=Data/tutor.trim.contigs.good.count_table)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to map the reads onto the reference sequence database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\\. Preparation of the most current Silva reference alignment files for mapping\n",
    "The goal of this step is to reduce the size of the full alignment flatfile to reduce the memory footprint, and to accelerate the mapping of the contigs onto the sequences within. The file `Silva.nr_v128.tgz` contains two files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- pschloss/staff 19877941 2017-03-22 08:37 silva.nr_v128.tax\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "-rw-r--r-- pschloss/staff 9554447505 2017-03-22 08:31 silva.nr_v128.align\n",
      "-rw-r--r-- pschloss/staff      14662 2017-03-22 08:43 README.md\n"
     ]
    }
   ],
   "source": [
    "tar tvf Silva.nr_v128.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `silva.nr_v128.tax` : the smaller taxonomy list for the sequences\n",
    "- `silva.nr_v128.align` : the alignment file (9111.8MB)\n",
    "\n",
    "We first extract the smaller file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silva.nr_v128.tax\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n"
     ]
    }
   ],
   "source": [
    "tar xvfz Silva.nr_v128.tgz silva.nr_v128.tax\n",
    "mv silva.nr_v128.tax Data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we reprocess `silva.nr_v128.align` to contain only the V4 region of the 16S rRNA data of the 104,711 unique sequences. According to the tutorial [1] out of the 50,000 column we only need the ones between 11894 and 25319. A blog post [2] explains how to modify the start/end to look for other variable domains (*not recommended*). The gist of it is that this file contains a bunch of rRNA sequences that are aligned *collectively*, but due to the alignemnt they get stretched out with a lot of gaps. Each new iteration of the Silva reference files may or may not move the start/stop sites around, so in order to find an approximate starting point we need to align a reference sequence (e.g. that of *E. coli*) to find the region (V4, V5, V34) of interest.\n",
    "\n",
    "Here I divert from the procedure suggested by the tutorial. Instead of using **Mothur** to reduce the size of the alignment file I will use a Bash script. The reason is the size of the uncomressed alignment (~8.9GB) which did not fit my Virtual Disk image. I first decompress and recompress the alignment file to get `silva.nr_v128.align.gz` in order to have it separated from the other files in the archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "silva.nr_v128.align\n",
      "-rw-rw-r-- 1 viktor viktor 323073676 Mar 23 14:44 silva.nr_v128.align.gz\n"
     ]
    }
   ],
   "source": [
    "tar xvfzO Silva.nr_v128.tgz silva.nr_v128.align | gzip -c - > silva.nr_v128.align.gz\n",
    "ls -la silva.nr_v128.align.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this produced a much smaller file (311MB).\n",
    "\n",
    "Next, I will cut out the needed column range and keep the FASTA headers. To keep it economical (small) I use process substitution and combine two streams with `paste`. Even though I *really* wanted, I cannot compress the resulting file for the next step, because - for some mysterious reasons - **Mothur** cannot process the alignment file as a `.align.gz`, even though for some functions (such as `make.contigs()`) using `gzip`ed files is definitely an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 viktor viktor 2581212091 Mar 23 15:07 silva.nr_v128.trunc.align\n"
     ]
    }
   ],
   "source": [
    "paste <(gunzip -c silva.nr_v128.align.gz | grep '^>') \\\n",
    "      <(gunzip -c silva.nr_v128.align.gz | grep -v '^>' \\\n",
    "                                         | cut -c 11894-25319 | tr -d '.') \\\n",
    "      -d '\\n' > Data/silva.nr_v128.trunc.align\n",
    "ls -la Data/silva.nr_v128.trunc.align"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even this truncated file is 2461.6MB... *(I was thinking about creating a named pipe to the compressed version, but I don't know if **Mothur** needs random access to the file.)*\n",
    "\n",
    "Moving on. We will first look at some general statistics of the alignment file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta=Data/silva.nr_v128.trunc.align, processors=2)\n",
      "Using 2 processors.\n",
      "mothur > set.logfile(name=mothur.log, append=T)\n",
      "mothur > summary.seqs(fasta=Data/silva.nr_v128.trunc.align, processors=2)\n",
      "Using 2 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1\t9877\t219\t0\t3\t1\n",
      "2.5%-tile:\t2\t13426\t292\t0\t4\t4767\n",
      "25%-tile:\t2\t13426\t293\t0\t4\t47666\n",
      "Median: \t2\t13426\t293\t0\t5\t95331\n",
      "75%-tile:\t2\t13426\t293\t0\t5\t142996\n",
      "97.5%-tile:\t2\t13426\t459\t1\t6\t185895\n",
      "Maximum:\t4226\t13426\t1521\t5\t16\t190661\n",
      "Mean:\t2.27349\t13425.8\t309.098\t0.048164\t4.75374\n",
      "# of Seqs:\t190661\n",
      "Output File Names: \n",
      "Data/silva.nr_v128.trunc.summary\n",
      "It took 27 secs to summarize 190661 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta=Data/silva.nr_v128.trunc.align,\n",
    "                     processors=2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have successfully reduced the 50,000 column wide alignment database which has 190,661 sequences.\n",
    "\n",
    "## 5\\. Align contigs onto the references\n",
    "\n",
    "This is equivalent of mapping the reads / contigs onto a reference genome, except here we have a lot of references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > align.seqs(fasta=Data/tutor.trim.contigs.good.unique.fasta, reference=Data/silva.nr_v128.trunc.align)\n",
      "Using 1 processors.\n",
      "Reading in the Data/silva.nr_v128.trunc.align template sequences...\tDONE.\n",
      "It took 89 to read  190661 sequences.\n",
      "Aligning sequences from Data/tutor.trim.contigs.good.unique.fasta ...\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1774\n",
      "It took 16 secs to align 1774 sequences.\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.align\n",
      "Data/tutor.trim.contigs.good.unique.align.report\n"
     ]
    }
   ],
   "source": [
    "Mothur \"align.seqs(fasta=Data/tutor.trim.contigs.good.unique.fasta,\n",
    "                   reference=Data/silva.nr_v128.trunc.align)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we aligned only 1,774 unique contigs instead of the 8,737 thanks to the deduplication step before.\n",
    "Let's see some statistics on the aligned sequences. The resulting FASTA file contains the contigs aligned onto the reference with spacing indicated by \"`.`\" characters, gaps indicated by \"`-`\" as ususal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.align, count=Data/tutor.trim.contigs.good.count_table, processors=2)\n",
      "Using 2 processors.\n",
      "mothur > set.logfile(name=mothur.log, append=T)\n",
      "mothur > summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.align, count=Data/tutor.trim.contigs.good.count_table, processors=2)\n",
      "Using 2 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1969\t11550\t251\t0\t3\t1\n",
      "2.5%-tile:\t1969\t11551\t252\t0\t3\t219\n",
      "25%-tile:\t1969\t11551\t252\t0\t4\t2185\n",
      "Median: \t1969\t11551\t253\t0\t4\t4369\n",
      "75%-tile:\t1969\t11551\t253\t0\t5\t6553\n",
      "97.5%-tile:\t1969\t11551\t254\t0\t6\t8519\n",
      "Maximum:\t1978\t11554\t255\t0\t6\t8737\n",
      "Mean:\t1969\t11551\t252.699\t0\t4.48083\n",
      "# of unique seqs:\t1774\n",
      "total # of seqs:\t8737\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.summary\n",
      "It took 0 secs to summarize 8737 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.align,\n",
    "                     count=Data/tutor.trim.contigs.good.count_table,\n",
    "                     processors=2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary shows that the contigs mapped onto more or less the same regions. We do not need additional filtering we can carry on with our analysis. The tutorial [1] has here an additional filtering step to remove the contigs that do not map like the bulk of them.\n",
    "\n",
    "We know that the aligned contigs mostly overlap, we remove the overhangs (spacer \"`.`\" to reduce file size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > filter.seqs(fasta= Data/tutor.trim.contigs.good.unique.align, vertical=T, trump=.)\n",
      "Using 1 processors.\n",
      "Creating Filter... \n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1774\n",
      "Running Filter... \n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1774\n",
      "Length of filtered alignment: 307\n",
      "Number of columns removed: 13119\n",
      "Length of the original alignment: 13426\n",
      "Number of sequences used to construct filter: 1774\n",
      "Output File Names: \n",
      "Data/tutor.filter\n",
      "Data/tutor.trim.contigs.good.unique.filter.fasta\n"
     ]
    }
   ],
   "source": [
    "Mothur \"filter.seqs(fasta= Data/tutor.trim.contigs.good.unique.align, vertical=T, trump=.)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting file `tutor.trim.contigs.good.unique.filter.fasta` contains the contigs mapped onto a reference genome file, aligned, just like as in an MSA, reduced from 13,426 column alignment to 307 column alignment.\n",
    "\n",
    "Since we have trimmed the ends of the aligned contigs, some of them may be redundant. Next, we make sure that we only deal with unique contigs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > unique.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.fasta, count=Data/tutor.trim.contigs.good.count_table)\n",
      "1000\t995\n",
      "1774\t1750\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.count_table\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.fasta\n"
     ]
    }
   ],
   "source": [
    "Mothur \"unique.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.fasta,\n",
    "                    count=Data/tutor.trim.contigs.good.count_table)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.fasta, count=Data/tutor.trim.contigs.good.unique.filter.count_table)\n",
      "Using 1 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1\t307\t248\t0\t3\t1\n",
      "2.5%-tile:\t1\t307\t249\t0\t3\t219\n",
      "25%-tile:\t1\t307\t249\t0\t4\t2185\n",
      "Median: \t1\t307\t250\t0\t4\t4369\n",
      "75%-tile:\t1\t307\t250\t0\t5\t6553\n",
      "97.5%-tile:\t1\t307\t251\t0\t6\t8519\n",
      "Maximum:\t1\t307\t252\t0\t6\t8737\n",
      "Mean:\t1\t307\t249.701\t0\t4.48083\n",
      "# of unique seqs:\t1750\n",
      "total # of seqs:\t8737\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.summary\n",
      "It took 0 secs to summarize 8737 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.fasta,\n",
    "                     count=Data/tutor.trim.contigs.good.unique.filter.count_table)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique sequences has reduced from 1774 to 1750, but these refer to all the contigs that we initially had (8,737).\n",
    "\n",
    "## 6\\. Removing sequencing errors by means of pre-clustering\n",
    "\n",
    "By preclustering contigs that have 1bp difference per 100bp the sequencing data can be further reduced with minor loss of information. The algorithm sorts the contigs by abundance and merges those that are close to one another and up to 2bp dissimilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > pre.cluster(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.fasta, count=Data/tutor.trim.contigs.good.unique.filter.count_table, diffs=2)\n",
      "Using 1 processors.\n",
      "Processing group F3D150:\n",
      "0\t1194\t48\n",
      "100\t681\t561\n",
      "200\t596\t646\n",
      "300\t576\t666\n",
      "400\t565\t677\n",
      "500\t557\t685\n",
      "600\t546\t696\n",
      "700\t542\t700\n",
      "800\t538\t704\n",
      "900\t535\t707\n",
      "1000\t533\t709\n",
      "1100\t531\t711\n",
      "1200\t531\t711\n",
      "1242\t531\t711\n",
      "Total number of sequences before pre.cluster was 1242.\n",
      "pre.cluster removed 711 sequences.\n",
      "It took 0 secs to cluster 1242 sequences.\n",
      "Processing group Mock:\n",
      "0\t468\t54\n",
      "100\t135\t387\n",
      "200\t133\t389\n",
      "300\t129\t393\n",
      "400\t125\t397\n",
      "500\t124\t398\n",
      "522\t124\t398\n",
      "Total number of sequences before pre.cluster was 522.\n",
      "pre.cluster removed 398 sequences.\n",
      "It took 0 secs to cluster 522 sequences.\n",
      "It took 1 secs to run pre.cluster.\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.fasta\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.count_table\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.F3D150.map\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.Mock.map\n"
     ]
    }
   ],
   "source": [
    "Mothur \"pre.cluster(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.fasta,\n",
    "                    count=Data/tutor.trim.contigs.good.unique.filter.count_table,\n",
    "                    diffs=2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process removed all together 398 + 722 = 1120 contigs. That further reduced the redundancy in our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\\.Removing chimeras\n",
    "\n",
    "During the contig assembly there may have been chimeric contigs produced that we want to remove. These are contigs produced by combining the wrong read pairs into a contig. This uses the external tool `vsearch` that we needed to install separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vsearch v2.4.2_linux_x86_64, 3.9GB RAM, 3 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file Data/tutor.trim.contigs.good.unique.filter.unique.precluster.temp 100%  \n",
      "132581 nt in 531 seqs, min 248, max 252, avg 250\n",
      "Masking 100%  \n",
      "Sorting by abundance 100%\n",
      "Counting unique k-mers 100%  \n",
      "Detecting chimeras 100%  \n",
      "Found 268 (50.5%) chimeras, 256 (48.2%) non-chimeras,\n",
      "and 7 (1.3%) borderline sequences in 531 unique sequences.\n",
      "Taking abundance information into account, this corresponds to\n",
      "474 (10.3%) chimeras, 4117 (89.4%) non-chimeras,\n",
      "and 13 (0.3%) borderline sequences in 4604 total sequences.\n",
      "vsearch v2.4.2_linux_x86_64, 3.9GB RAM, 3 cores\n",
      "https://github.com/torognes/vsearch\n",
      "\n",
      "Reading file Data/tutor.trim.contigs.good.unique.filter.unique.precluster.temp 100%  \n",
      "30988 nt in 124 seqs, min 249, max 252, avg 250\n",
      "Masking 100% \n",
      "Sorting by abundance 100%\n",
      "Counting unique k-mers 100% \n",
      "Detecting chimeras 100%  \n",
      "Found 62 (50.0%) chimeras, 60 (48.4%) non-chimeras,\n",
      "and 2 (1.6%) borderline sequences in 124 unique sequences.\n",
      "Taking abundance information into account, this corresponds to\n",
      "77 (1.9%) chimeras, 4054 (98.1%) non-chimeras,\n",
      "and 2 (0.0%) borderline sequences in 4133 total sequences.\n",
      "mothur > chimera.vsearch(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.fasta, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.count_table, dereplicate=T)\n",
      "Using 1 processors.\n",
      "/usr/local/bin/mothurvsearch file does not exist. Checking path... \n",
      "Found vsearch in your path, using /usr/local/bin/vsearch\n",
      "Checking sequences from Data/tutor.trim.contigs.good.unique.filter.unique.precluster.fasta ...\n",
      "It took 1 secs to check 531 sequences from group F3D150.\n",
      "It took 0 secs to check 124 sequences from group Mock.\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.chimeras\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos\n"
     ]
    }
   ],
   "source": [
    "Mothur \"chimera.vsearch(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.fasta,\n",
    "                        count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.count_table,\n",
    "                        dereplicate=T)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will remove the chimeras from the count file, but not from the FASTA file, we need to do that manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > remove.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.fasta, accnos=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos)\n",
      "Removed 330 sequences from your fasta file.\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta\n"
     ]
    }
   ],
   "source": [
    "Mothur \"remove.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.fasta,\n",
    "                    accnos=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.accnos)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check how much our overall contig database has shrunk by removing some of the unique contigs as chimeras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table)\n",
      "Using 1 processors.\n",
      "\t\tStart\tEnd\tNBases\tAmbigs\tPolymer\tNumSeqs\n",
      "Minimum:\t1\t307\t248\t0\t3\t1\n",
      "2.5%-tile:\t1\t307\t249\t0\t3\t205\n",
      "25%-tile:\t1\t307\t249\t0\t4\t2047\n",
      "Median: \t1\t307\t250\t0\t4\t4094\n",
      "75%-tile:\t1\t307\t250\t0\t5\t6140\n",
      "97.5%-tile:\t1\t307\t251\t0\t6\t7982\n",
      "Maximum:\t1\t307\t252\t0\t6\t8186\n",
      "Mean:\t1\t307\t249.715\t0\t4.48778\n",
      "# of unique seqs:\t312\n",
      "total # of seqs:\t8186\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.summary\n",
      "It took 0 secs to summarize 8186 sequences.\n"
     ]
    }
   ],
   "source": [
    "Mothur \"summary.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta,\n",
    "                     count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduction is about 6.3% (from 8,737), the tutorial had a 8.1% reduction, this is even less, I believe it is a good sign.\n",
    "\n",
    "## 8. Classification of contigs into OTUs\n",
    "Now that we have reduced the number of contigs and removed potential read errors, we can go ahead and classify the sequences using the Bayesian classifier. The reference files for that are the `trainset16_022016.pds.fasta` and the taxonomy information `trainset16_022016.pds.tax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > classify.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, reference=PDS/trainset16_022016.pds.fasta, taxonomy=PDS/trainset16_022016.pds.tax, cutoff=80)\n",
      "Using 1 processors.\n",
      "Generating search database...    DONE.\n",
      "It took 6 seconds generate search database. \n",
      "Reading in the PDS/trainset16_022016.pds.tax taxonomy...\tDONE.\n",
      "Calculating template taxonomy tree...     DONE.\n",
      "Calculating template probabilities...     DONE.\n",
      "It took 18 seconds get probabilities. \n",
      "Classifying sequences from Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta ...\n",
      "Processing sequence: 100\n",
      "Processing sequence: 200\n",
      "Processing sequence: 300\n",
      "Processing sequence: 312\n",
      "It took 5 secs to classify 312 sequences.\n",
      "It took 0 secs to create the summary file for 312 sequences.\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.tax.summary\n"
     ]
    }
   ],
   "source": [
    "Mothur \"classify.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta,\n",
    "                      count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table,\n",
    "                      reference=PDS/trainset16_022016.pds.fasta, \n",
    "                      taxonomy=PDS/trainset16_022016.pds.tax, \n",
    "                      cutoff=80)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not accurate enough to pinpoint the actual species with high confidence, this is where we generate the OTUs. We set the OTU clustering to the level of Order (4), to increase the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > cluster.split(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, taxonomy=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy, splitmethod=classify, taxlevel=4, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "[NOTE]: Default clustering method has changed to opti. To use average neighbor, set method=average.\n",
      "Splitting the file...\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.0.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "100\t0\n",
      "176\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.0.dist\n",
      "It took 0 seconds to calculate the distances for 177 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.1.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "7\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.1.dist\n",
      "It took 0 seconds to calculate the distances for 8 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.2.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "59\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.2.dist\n",
      "It took 0 seconds to calculate the distances for 60 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.3.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "1\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.3.dist\n",
      "It took 0 seconds to calculate the distances for 2 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.4.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "5\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.4.dist\n",
      "It took 0 seconds to calculate the distances for 6 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.5.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "11\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.5.dist\n",
      "It took 0 seconds to calculate the distances for 12 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.6.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "12\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.6.dist\n",
      "It took 0 seconds to calculate the distances for 13 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.7.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "1\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.7.dist\n",
      "It took 0 seconds to calculate the distances for 2 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.8.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "2\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.8.dist\n",
      "It took 0 seconds to calculate the distances for 3 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.9.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "8\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.9.dist\n",
      "It took 0 seconds to calculate the distances for 9 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.10.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "5\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.10.dist\n",
      "It took 0 seconds to calculate the distances for 6 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.11.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "1\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.11.dist\n",
      "It took 0 seconds to calculate the distances for 2 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.12.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "3\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.12.dist\n",
      "It took 0 seconds to calculate the distances for 4 sequences.\n",
      "/******************************************/\n",
      "Running command: dist.seqs(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.13.temp, processors=1, cutoff=0.03)\n",
      "Using 1 processors.\n",
      "/******************************************/\n",
      "0\t0\n",
      "1\t0\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.13.dist\n",
      "It took 0 seconds to calculate the distances for 2 sequences.\n",
      "It took 0 seconds to split the distance file.\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.2.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "69\t2053\t6\t17\t0.802326\t0.997086\t0.92\t0.991787\t0.08\t0.989277\t0.853753\t0.857143\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.0.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "68\t16568\t9\t8\t0.894737\t0.999457\t0.883117\t0.999517\t0.116883\t0.998979\t0.888395\t0.888889\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.5.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "15\t130\t0\t8\t0.652174\t1\t1\t0.942029\t0\t0.947712\t0.783815\t0.789474\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.1.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "9\t82\t0\t0\t1\t1\t1\t1\t0\t1\t1\t1\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.6.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "8\t163\t0\t0\t1\t1\t1\t1\t0\t1\t1\t1\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.12.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "3\t41\t0\t1\t0.75\t1\t1\t0.97619\t0\t0.977778\t0.855653\t0.857143\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.4.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t63\t0\t3\t0\t1\t0\t0.954545\t0\t0.954545\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.8.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t33\t0\t3\t0\t1\t0\t0.916667\t0\t0.916667\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.10.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t65\t0\t1\t0\t1\t0\t0.984848\t0\t0.984848\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.3.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t27\t0\t1\t0\t1\t0\t0.964286\t0\t0.964286\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.7.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t27\t0\t1\t0\t1\t0\t0.964286\t0\t0.964286\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.9.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t104\t0\t1\t0\t1\t0\t0.990476\t0\t0.990476\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.13.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t27\t0\t1\t0\t1\t0\t0.964286\t0\t0.964286\t0\t0\t\n",
      "Clustering Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta.11.dist\n",
      "tp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0\t27\t0\t1\t0\t1\t0\t0.964286\t0\t0.964286\t0\t0\t\n",
      "It took 0 seconds to cluster\n",
      "Merging the clustered files...\n",
      "It took 0 seconds to merge.\n",
      "/******************************************/\n",
      "Running command: sens.spec(cutoff=0.03, list=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.list, column=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.dist, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table)\n",
      "NOTE: sens.spec assumes that only unique sequences were used to generate the distance matrix.\n",
      "It took 0 to run sens.spec.\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.sensspec\n",
      "/******************************************/\n",
      "Done.\n",
      "label\tcutoff\ttp\ttn\tfp\tfn\tsensitivity\tspecificity\tppv\tnpv\tfdr\taccuracy\tmcc\tf1score\n",
      "0.03\t0.03\t172\t48283\t15\t46\t0.789\t0.9997\t0.9198\t0.999\t0.08021\t0.9987\t0.8513\t0.8494\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.dist\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.list\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.sensspec\n"
     ]
    }
   ],
   "source": [
    "Mothur \"cluster.split(fasta=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.fasta,\n",
    "                      count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table,\n",
    "                      taxonomy=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy,\n",
    "                      splitmethod=classify, \n",
    "                      taxlevel=4, \n",
    "\n",
    "cutoff=0.03)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is very verbose, at the end we have list of OTUs, into which each the reads are classified into.\n",
    "\n",
    "The previous step produced OTU per contig, but it did not care about the sample source. We need to assign these reads to the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > classify.otu(list=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.list, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, taxonomy=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy, label=0.03)\n",
      "0.03\t199\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.0.03.cons.taxonomy\n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.0.03.cons.tax.summary\n"
     ]
    }
   ],
   "source": [
    "Mothur \"classify.otu(list=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.list,\n",
    "                     count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table,\n",
    "                     taxonomy=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.pds.wang.taxonomy,\n",
    "                     label=0.03)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtain the quantitative information: we calculate how many reads belonged into each OTU per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mothur > make.shared(list=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.list, count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table, label=0.03)\n",
      "0.03\n",
      "Output File Names: \n",
      "Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.shared\n"
     ]
    }
   ],
   "source": [
    "Mothur \"make.shared(list=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.pick.opti_mcc.unique_list.list,\n",
    "                    count=Data/tutor.trim.contigs.good.unique.filter.unique.precluster.denovo.vsearch.pick.count_table,\n",
    "                    label=0.03)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://www.mothur.org/wiki/MiSeq_SOP\n",
    "\n",
    "[2] http://blog.mothur.org/2016/07/07/Customization-for-your-region/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
